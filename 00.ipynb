{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合併並儲存完成!\n"
     ]
    }
   ],
   "source": [
    "# 定義 CSV 文件所在的資料夾路徑\n",
    "folder_path = '/home/xcliu/AS/CutUSGSignal'\n",
    "\n",
    "# 初始化一個空的 DataFrame 來儲存合併後的數據\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# 遍歷資料夾中的所有 CSV 文件\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        # 讀取每個 CSV 文件並將其追加到 DataFrame 中\n",
    "        df = pd.read_csv(file_path)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "# 儲存合併後的 DataFrame 為 PKL 文件\n",
    "combined_df.to_pickle('usg_data.pkl')\n",
    "\n",
    "print(\"合併並儲存完成!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "usg_data = pd.read_pickle('/home/xcliu/AS/as_pm/usg_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义均值和标准差\n",
    "scaler_filepath = os.path.join(\"./data/scaler\", 'scaler_info.json')\n",
    "with open(scaler_filepath, \"r\") as json_file:\n",
    "    scaler_info = json.load(json_file)  \n",
    "mean = scaler_info['mean']\n",
    "std_dev = scaler_info['std']\n",
    "\n",
    "# 对数据进行标准化\n",
    "usg_data['USGSignal_scaled'] = (usg_data[['USGSignal']].values - mean) / std_dev\n",
    "\n",
    "# 如果你想要保存标准化后的数据，可以这样做：\n",
    "# usg_data_standardized.to_pickle('/path/to/save/standardized_usg_data.pkl')\n",
    "\n",
    "# print(usg_data_standardized.head())  # 查看标准化后的数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = usg_data.groupby(['filename', 'Group'])\n",
    "\n",
    "output_dir = 'data'  # 指定存儲 JSON 文件的目錄\n",
    "# os.makedirs(output_dir, exist_ok=True)  # 創建目錄（如果不存在的話）\n",
    "\n",
    "# 初始化標準化對象\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 先對所有數據的 'USGSignal' 進行標準化\n",
    "usg_signals = usg_data[['USGSignal']].values\n",
    "usg_signals_scaled = scaler.fit_transform(usg_signals)\n",
    "\n",
    "# 用標準化的數據更新原始數據\n",
    "usg_data['USGSignal_scaled'] = usg_signals_scaled\n",
    "\n",
    "# 獲取 scaler 的 mean 和 std\n",
    "scaler_mean = scaler.mean_[0]\n",
    "scaler_std = scaler.scale_[0]\n",
    "\n",
    "# 將 mean 和 std 儲存為 JSON 文件，方便日後返回原數據\n",
    "scaler_info = {\n",
    "    'mean': scaler_mean,\n",
    "    'std': scaler_std\n",
    "}\n",
    "scaler_filepath = os.path.join(output_dir, 'scaler_info.json')\n",
    "with open(scaler_filepath, 'w') as json_file:\n",
    "    json.dump(scaler_info, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = usg_data.groupby(['filename', 'Group'])\n",
    "\n",
    "output_dir = 'data/validation'  # 指定存儲 JSON 文件的目錄\n",
    "# os.makedirs(output_dir, exist_ok=True)  # 創建目錄（如果不存在的話）\n",
    "\n",
    "life_time = 1\n",
    "for (seq_id, sub_seq_id), group in grouped:\n",
    "    sequence_features = group[['USGSignal_scaled']].values.flatten().tolist()\n",
    "    label = life_time / 9000  # 900k\n",
    "\n",
    "    # sequence_tensor = torch.tensor(sequence_features, dtype=torch.float32)\n",
    "    # label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "    \n",
    "    # # Convert tensors to lists and scalars\n",
    "    # sequence_data = sequence_tensor.tolist()\n",
    "    # label_data = label_tensor.item()\n",
    "    \n",
    "    # Create a dictionary to store sequence and label data\n",
    "    data = {\n",
    "        'sequence': sequence_features,\n",
    "        'label': label\n",
    "    }\n",
    "    \n",
    "    # Create a filename for each group\n",
    "    filename = f\"cap3_{seq_id}_{sub_seq_id}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Save to JSON file\n",
    "    with open(filepath, 'w') as json_file:\n",
    "        json.dump(data, json_file)\n",
    "    \n",
    "    life_time += 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = usg_data.groupby(['filename', 'Group'])\n",
    "\n",
    "for (seq_id, sub_seq_id), group in grouped:\n",
    "    print(f\"Group: sequence_id={seq_id}, sub_sequence_id={sub_seq_id}\")\n",
    "\n",
    "# sequences = []\n",
    "# labels = []\n",
    "# life_time = 1\n",
    "# for (seq_id, sub_seq_id), group in grouped:\n",
    "#     sequence_features = group[['USGSignal']].values.flatten().tolist()\n",
    "#     label = life_time / 9000  # 900k\n",
    "\n",
    "#     sequence_tensor = torch.tensor(sequence_features, dtype=torch.float32)\n",
    "#     label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "    \n",
    "#     # sequences.append(sequence_tensor.tolist())\n",
    "#     # labels.append(label_tensor.tolist())\n",
    "    \n",
    "#     life_time += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 檢查是否有sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义要检查的目录\n",
    "data_dir = \"./data\"  # 替换为你的目录路径\n",
    "\n",
    "# 遍历目录下的所有文件\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        try:\n",
    "            # 打开并加载JSON文件\n",
    "            with open(filepath, 'r') as file:\n",
    "                data = json.load(file)\n",
    "            \n",
    "            # 检查' sequence'是否在JSON数据中\n",
    "            if 'sequence' not in data:\n",
    "                print(f\"'sequence' key not found in: {filename}\")\n",
    "        \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 移動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 定义源目录和目标目录\n",
    "source_dir = \"./data\"\n",
    "target_dir = os.path.join(source_dir, \"training\")\n",
    "\n",
    "# 如果目标目录不存在，则创建它\n",
    "# if not os.path.exists(target_dir):\n",
    "#     os.makedirs(target_dir)\n",
    "\n",
    "# 遍历源目录下的所有文件\n",
    "for filename in os.listdir(source_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        # 构建完整的源文件路径和目标文件路径\n",
    "        source_path = os.path.join(source_dir, filename)\n",
    "        target_path = os.path.join(target_dir, filename)\n",
    "        \n",
    "        # 移动文件到目标目录\n",
    "        shutil.move(source_path, target_path)\n",
    "        # print(f\"Moved: {filename} to {target_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 定义目标目录\n",
    "target_dir = \"./data/training\"\n",
    "\n",
    "# 遍历目标目录下的所有文件\n",
    "for filename in os.listdir(target_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        # 构建完整的旧文件路径和新文件路径\n",
    "        old_filepath = os.path.join(target_dir, filename)\n",
    "        new_filename = \"cap1_\" + filename\n",
    "        new_filepath = os.path.join(target_dir, new_filename)\n",
    "        \n",
    "        # 重命名文件\n",
    "        os.rename(old_filepath, new_filepath)\n",
    "        # print(f\"Renamed: {filename} to {new_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.0669816558396192\n",
      "Standard Deviation: 0.16946513850743108\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean and standard deviation\n",
    "mean = np.mean(usg_data['USGSignal'])\n",
    "std_dev = np.std(usg_data['USGSignal'])\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Standard Deviation: {std_dev}\")\n",
    "\n",
    "# Optionally, save these values to a file\n",
    "scaler_params = {\n",
    "    'mean': mean,\n",
    "    'std_dev': std_dev\n",
    "}\n",
    "\n",
    "# with open('scaler_params.json', 'w') as file:\n",
    "#     json.dump(scaler_params, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': np.float64(0.0669816558396192),\n",
       " 'std_dev': np.float64(0.16946513850743108)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "usg_data = {\n",
    "    \"sequences\": sequences,\n",
    "    \"labels\": labels\n",
    "}\n",
    "\n",
    "# 保存為 JSON 文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_260430/727725011.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_data = torch.load('usg_data.pth')\n"
     ]
    }
   ],
   "source": [
    "# 保存 sequences 和 labels\n",
    "# torch.save({'sequences': sequences, 'labels': labels}, 'usg_data.pth')\n",
    "\n",
    "# 读取数据\n",
    "loaded_data = torch.load('usg_data.pth')\n",
    "sequences = loaded_data['sequences']\n",
    "torch.save(sequences, 'sequences.pth')\n",
    "torch.save(labels, 'label.pth')\n",
    "labels = loaded_data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2457,  0.2118,  0.1550, -0.0696, -0.0562,  0.1987,  0.1761,  0.1581,\n",
      "        -0.0531, -0.0971, -0.0983, -0.1187, -0.1477, -0.1175, -0.0857, -0.0806,\n",
      "         0.1782,  0.2008,  0.2011,  0.2548,  0.2307,  0.1956,  0.1502, -0.1013,\n",
      "        -0.1410, -0.1535, -0.1477, -0.1892, -0.1492, -0.1041, -0.0571,  0.2307,\n",
      "         0.2383,  0.2447,  0.2341,  0.2600,  0.2100,  0.1636, -0.0668, -0.1099,\n",
      "        -0.2103, -0.1956, -0.1828, -0.2026, -0.1636, -0.1059,  0.1883,  0.2673,\n",
      "         0.2942,  0.2884,  0.2649,  0.2615,  0.2210,  0.1535, -0.1126, -0.1620,\n",
      "        -0.2332, -0.2362, -0.2133, -0.1810, -0.1730, -0.1041,  0.1901,  0.2344,\n",
      "         0.2756,  0.3418,  0.3253,  0.2972,  0.2469,  0.2210,  0.1532, -0.1068,\n",
      "        -0.1685, -0.2042, -0.2698, -0.2679, -0.2399, -0.1913, -0.1700, -0.0885,\n",
      "         0.1724,  0.2426,  0.2927,  0.3220,  0.3339,  0.3079,  0.2692,  0.2197,\n",
      "         0.1541, -0.0958, -0.1385, -0.1639, -0.1703, -0.1620, -0.1428, -0.1056,\n",
      "        -0.0635,  0.1834,  0.2167,  0.2341,  0.2237,  0.2090,  0.1858, -0.1019,\n",
      "        -0.1160, -0.1321, -0.1343, -0.1294, -0.0995, -0.0699,  0.1532,  0.1868,\n",
      "         0.2072,  0.2072,  0.2014,  0.1849,  0.1520, -0.0641, -0.1019, -0.1227,\n",
      "        -0.1355, -0.1282, -0.1099, -0.0800, -0.0503,  0.1773,  0.2035,  0.2115,\n",
      "         0.2158,  0.2002,  0.1709, -0.0842, -0.1227, -0.1349, -0.1398, -0.1306,\n",
      "        -0.1041, -0.0708,  0.1523,  0.1886,  0.2063,  0.2133,  0.2066,  0.1828,\n",
      "         0.1620, -0.0671, -0.1035, -0.1300, -0.1376, -0.1337, -0.1208, -0.0931,\n",
      "        -0.0528,  0.1679,  0.2072,  0.2124,  0.2133,  0.2069,  0.1721, -0.0552,\n",
      "        -0.0812, -0.1184, -0.1358, -0.1367, -0.1361, -0.1105, -0.0763,  0.1511,\n",
      "         0.1801,  0.2057,  0.2136,  0.2066,  0.1871,  0.1620, -0.0595, -0.1004,\n",
      "        -0.1279, -0.1413, -0.1392, -0.1202, -0.0989, -0.0607,  0.1669,  0.1981,\n",
      "         0.2200,  0.2176,  0.2063,  0.1892, -0.0934, -0.1154, -0.1361, -0.1434,\n",
      "        -0.1312, -0.1138, -0.0845,  0.1837,  0.2057,  0.2161,  0.2109,  0.1953,\n",
      "         0.1666, -0.0525, -0.0949, -0.1245, -0.1398, -0.1434, -0.1300, -0.1016,\n",
      "        -0.0766,  0.1596,  0.1929,  0.2115,  0.2203,  0.2063,  0.1895, -0.0751,\n",
      "        -0.1093, -0.1328, -0.1446, -0.1404, -0.1184, -0.0885, -0.0537,  0.1764,\n",
      "         0.2017,  0.2142,  0.2109,  0.1974,  0.1715, -0.0870, -0.1239, -0.1379,\n",
      "        -0.1392, -0.1279, -0.1044, -0.0684,  0.1523,  0.1944,  0.2084,  0.2151,\n",
      "         0.2124,  0.1834,  0.1556, -0.0653, -0.1129, -0.1309, -0.1395, -0.1343,\n",
      "        -0.1215, -0.0937, -0.0513,  0.1709,  0.1993,  0.2139,  0.2158,  0.1981,\n",
      "         0.1767, -0.0845, -0.1218, -0.1328, -0.1425, -0.1300, -0.1126, -0.0778,\n",
      "         0.1862,  0.2106,  0.2142,  0.2100,  0.1782,  0.1639, -0.0586, -0.1056,\n",
      "        -0.1306, -0.1434, -0.1389, -0.1273, -0.1016, -0.0629,  0.1679,  0.1938,\n",
      "         0.2087,  0.2139,  0.2042,  0.1797,  0.1532, -0.0784, -0.1132, -0.1379,\n",
      "        -0.1431, -0.1389, -0.1135, -0.0830,  0.1791,  0.2011,  0.2234,  0.2112,\n",
      "         0.1987,  0.1587, -0.0558, -0.1007, -0.1239, -0.1404, -0.1431, -0.1300,\n",
      "        -0.1025, -0.0717,  0.1581,  0.1913,  0.2081,  0.2081,  0.2045,  0.1834,\n",
      "         0.1553, -0.0714, -0.1086, -0.1306, -0.1410, -0.1385, -0.1160, -0.0925,\n",
      "        -0.0555,  0.1770,  0.1968,  0.2109,  0.1947,  0.2008,  0.1697, -0.0909,\n",
      "        -0.1193, -0.1395, -0.1444, -0.1303, -0.1123, -0.0748,  0.1797,  0.2029,\n",
      "         0.2048,  0.2063,  0.1858,  0.1636, -0.0635, -0.0995, -0.1282, -0.1416,\n",
      "        -0.1300, -0.1227, -0.0974, -0.0555,  0.1623,  0.2023,  0.2090,  0.2155,\n",
      "         0.1868,  0.1706, -0.0864, -0.1160, -0.1370, -0.1404, -0.1355, -0.1193,\n",
      "        -0.0888,  0.1743,  0.2002,  0.2084,  0.2100,  0.1926,  0.1663, -0.0519,\n",
      "        -0.0919, -0.1212, -0.1358, -0.1422, -0.1257, -0.1135, -0.0751,  0.1813,\n",
      "         0.2078,  0.2118,  0.1993,  0.1666,  0.1532, -0.0671, -0.1025, -0.1288,\n",
      "        -0.1401, -0.1367, -0.1208, -0.0977, -0.0617,  0.1633,  0.1941])\n",
      "tensor(0.0001)\n"
     ]
    }
   ],
   "source": [
    "print(sequences[0])\n",
    "print(labels[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "as_pm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
